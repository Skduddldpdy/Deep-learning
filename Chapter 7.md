## 7
- Universal Approximation Theorem 보편 근사 정리 <br>
  : 인공신경망은 사실상 모든 함수를 표현할 수 있다
  > - 수많은 함수 중 MLP(다층 퍼셉트론)을 선택한 이유 <br>
  >   : 히든 레이어가 단 한층만 있어도 제한된 범위 안에서 어떤 연속함수든 나타낼 수 있다. <br>
  >   (단 히든 레이어는 비선형 함수, 출력 층은 선형 함수, 노드 개수 제한x)
  > - 어떤 연속 함수든 f(xW1+b1)W2+b2 이 식으로 다 표현 가능 (train loss를 딱 0으로 만들 수 있음)
  > - 이렇게 훈련된다는 것은 아니고 표현 가능일 뿐 FC layer는 굉장히 비효율적
  > - 미분만 가능하면 layer 언제든지 만들 수 있음
