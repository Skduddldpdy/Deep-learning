## 5-1
- MLP의 동작방식 : 웨이트 곱하고 바이어스와 함께 더하고 액티베이션의 반복
- 행렬과 백터의 표현
  > ![image](https://github.com/user-attachments/assets/7d25315b-5ab1-4b52-9175-08ccf7db8fd0)
  > ![image](https://github.com/user-attachments/assets/7f2e9177-78f8-4584-9f59-766a1d35d1b7)
- 비선형 액티베이션의 중요성 : 활성화 함수로 비선형 함수를 사용해야 깊어질수록 더 복잡한 함수를 만들 수 있음
  > - 선형 액티베이션을 사용한 층은 하나의 층으로 축약 <br>
  > - 선형함수 어떨 때 사용?
  >> 1) 회귀 문제에서 출력값의 범위가 제한되지 않기 위해 마지막 층에 선형 액테비에션 사용
  >> 2) 정보 손실 막기 위해 (노드 수가 줄어드는 레이어에서는 정보 손실이 클 수 있으므로 선형 액티베이션 사용)
> 1. 선형 액티베이션 : 복잡도 유지, 정보 손실 없음
> 2. 비선형 액티베이션 : 복잡도 증가, 정보 손실 발생
> 3. 노드 수 감소 : 정보 손실 발생 (차원 축소로 인한 정보 압축)
> 4. 노드 수 증가 : 정보 손실 없지만 새로운 정보 생성도 없음
## 5-2
- 역전파 Backpropagation
  : 깊은 인공신경망의 각 파라미터에 대한 Loss의 편미분을 효율적으로 계산하는 핵심 알고리즘
  : 출력층에서 시작하여 입력층 방향으로 계산 진행
  > - 연쇄 법칙 chain rule의 적용과 중간 계산 결과의 재사용
  > - path 전부 고려해서 더해줘야함
  > - 액(티베이션)웨(이트)*층수 액(티베이션)나(오는 값)
  > - forward propagation 한번 해서 값을 구해놓고 backpropagation을 통해 미분 값을 구하고 SGD 같은 거 사용
